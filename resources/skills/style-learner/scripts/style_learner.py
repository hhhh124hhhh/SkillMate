from typing import Dict, Any, List, Tuple
import json
import re
import os
from datetime import datetime
from collections import Counter
import sys

# Fix encoding issues on Windows
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

"""
å†™ä½œé£æ ¼å­¦ä¹ å·¥å…·

åŸºäºAIçš„å†™ä½œé£æ ¼å­¦ä¹ å·¥å…·ï¼Œèƒ½å¤Ÿåˆ†æç”¨æˆ·å†å²æ–‡ç« ï¼Œæå–å†™ä½œé£æ ¼ç‰¹å¾ï¼Œå¹¶åŸºäºé£æ ¼ç”Ÿæˆæ–°å†…å®¹

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. é£æ ¼åˆ†æï¼šå¤šç»´åº¦åˆ†æå†™ä½œé£æ ¼ç‰¹å¾
2. é£æ ¼æè¿°ç”Ÿæˆï¼šç”Ÿæˆç»“æ„åŒ–çš„é£æ ¼æè¿°æ–‡æ¡£
3. åŸºäºé£æ ¼ç”Ÿæˆï¼šç”Ÿæˆæ ‡é¢˜ã€æ¶¦è‰²å†…å®¹ã€åˆ›ä½œæ–‡ç« 
4. é£æ ¼åº“ç®¡ç†ï¼šä¿å­˜å’Œç®¡ç†å¤šä¸ªé£æ ¼ç‰ˆæœ¬

å…³é”®æ•°æ®ï¼š
- æœ€å°‘æ–‡ç« æ•°é‡ï¼š10ç¯‡
- æ¨èæ–‡ç« æ•°é‡ï¼š20-30ç¯‡
- æœ€ä½³æ–‡ç« æ•°é‡ï¼š50ç¯‡ä»¥ä¸Š
"""


def parse_articles(articles_text: str) -> List[Dict[str, str]]:
    """
    è§£æç”¨æˆ·æä¾›çš„æ–‡ç« æ–‡æœ¬

    Args:
        articles_text: æ–‡ç« æ–‡æœ¬ï¼ˆå¯èƒ½åŒ…å«å¤šç¯‡æ–‡ç« ï¼‰

    Returns:
        è§£æåçš„æ–‡ç« åˆ—è¡¨
    """
    # å°è¯•æŒ‰æ–‡ç« åˆ†éš”ç¬¦åˆ†å‰²
    # å¸¸è§åˆ†éš”ç¬¦ï¼š===ã€---ã€å¤šä¸ªæ¢è¡Œç­‰
    separators = [
        r'\n={3,}\n',
        r'\n-{3,}\n',
        r'\n\n\n+',
    ]

    articles = []

    # å°è¯•ä½¿ç”¨åˆ†éš”ç¬¦
    for separator in separators:
        parts = re.split(separator, articles_text)
        if len(parts) > 1:
            # æ£€æŸ¥æ˜¯å¦æœ‰æ•ˆåˆ†å‰²
            valid_parts = [p.strip() for p in parts if len(p.strip()) > 100]
            if len(valid_parts) >= 2:
                for i, part in enumerate(valid_parts):
                    articles.append({
                        "id": i + 1,
                        "content": part
                    })
                break

    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ†éš”ç¬¦ï¼Œå°è¯•æŒ‰æ ‡é¢˜åˆ†å‰²
    if not articles:
        lines = articles_text.split('\n')
        current_article = []
        article_count = 0

        for line in lines:
            # æ£€æµ‹æ ‡é¢˜ï¼ˆè¡Œé¦–æ— ç¼©è¿›ï¼Œé•¿åº¦é€‚ä¸­ï¼Œä¸æ˜¯çº¯æ•°å­—ï¼‰
            if (line.strip() and
                not line.startswith(' ') and
                len(line.strip()) < 100 and
                not line.strip().isdigit() and
                not re.match(r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[ã€.ï¼Œ]', line.strip())):

                if current_article:
                    article_count += 1
                    articles.append({
                        "id": article_count,
                        "content": '\n'.join(current_article)
                    })
                current_article = []

            current_article.append(line)

        # æ·»åŠ æœ€åä¸€ç¯‡æ–‡ç« 
        if current_article:
            article_count += 1
            articles.append({
                "id": article_count,
                "content": '\n'.join(current_article)
            })

    # å¦‚æœè¿˜æ˜¯æ²¡æœ‰åˆ†å‰²æˆåŠŸï¼Œå½“ä½œä¸€ç¯‡æ–‡ç« 
    if not articles:
        articles = [{
            "id": 1,
            "content": articles_text
        }]

    return articles


def extract_title(article_content: str) -> str:
    """
    æå–æ–‡ç« æ ‡é¢˜

    Args:
        article_content: æ–‡ç« å†…å®¹

    Returns:
        æ ‡é¢˜
    """
    lines = article_content.split('\n')

    for line in lines[:10]:  # åªæ£€æŸ¥å‰10è¡Œ
        line = line.strip()

        # è·³è¿‡ç©ºè¡Œ
        if not line:
            continue

        # è·³è¿‡ç‰¹æ®Šæ ‡è®°
        if line.startswith('#') or line.startswith('>') or line.startswith('```'):
            continue

        # æ£€æŸ¥æ˜¯å¦æ˜¯æ ‡é¢˜ï¼ˆä¸æ˜¯çº¯æ•°å­—ï¼Œä¸æ˜¯å¤ªçŸ­ï¼Œä¸æ˜¯å¤ªé•¿ï¼‰
        if (not line.isdigit() and
            len(line) >= 5 and
            len(line) <= 100 and
            not re.match(r'^\d+[ã€.ï¼Œ]', line)):

            return line

    return "æœªæ‰¾åˆ°æ ‡é¢˜"


def extract_opening(article_content: str) -> str:
    """
    æå–æ–‡ç« å¼€å¤´

    Args:
        article_content: æ–‡ç« å†…å®¹

    Returns:
        å¼€å¤´æ–‡æœ¬
    """
    lines = article_content.split('\n')

    opening_lines = []
    in_opening = True

    for line in lines:
        line = line.strip()

        # è·³è¿‡æ ‡é¢˜è¡Œ
        if len(opening_lines) == 0 and not line:
            continue

        # æ£€æŸ¥æ˜¯å¦ç¦»å¼€å¼€å¤´
        if not in_opening:
            break

        # å¦‚æœæ˜¯ç‰¹æ®Šæ ‡è®°ï¼ˆå¦‚##ï¼‰ï¼Œè®¤ä¸ºå¼€å¤´ç»“æŸ
        if line.startswith('#'):
            in_opening = False
            break

        # å¦‚æœå¼€å¤´è¶…è¿‡200å­—ï¼Œç»“æŸ
        if len(' '.join(opening_lines)) > 200:
            in_opening = False
            break

        # å¦‚æœæ˜¯ç©ºè¡Œä¸”å·²ç»æœ‰å†…å®¹ï¼Œå¯èƒ½è¿›å…¥æ­£æ–‡
        if not line and opening_lines:
            in_opening = False
            break

        opening_lines.append(line)

    return ' '.join(opening_lines[:5])  # æœ€å¤š5è¡Œ


def extract_content_body(article_content: str) -> List[str]:
    """
    æå–æ–‡ç« æ­£æ–‡æ®µè½

    Args:
        article_content: æ–‡ç« å†…å®¹

    Returns:
        æ­£æ–‡æ®µè½åˆ—è¡¨
    """
    lines = article_content.split('\n')

    paragraphs = []
    current_paragraph = []

    skip_opening = True
    opening_count = 0

    for line in lines:
        line = line.strip()

        # è·³è¿‡å¼€å¤´
        if skip_opening:
            if line:
                opening_count += 1
            if opening_count >= 5:
                skip_opening = False
            continue

        # è·³è¿‡ç©ºè¡Œ
        if not line:
            if current_paragraph:
                paragraphs.append(' '.join(current_paragraph))
                current_paragraph = []
            continue

        # è·³è¿‡ç‰¹æ®Šæ ‡è®°
        if line.startswith('#') or line.startswith('>') or line.startswith('```'):
            continue

        current_paragraph.append(line)

    # æ·»åŠ æœ€åä¸€ä¸ªæ®µè½
    if current_paragraph:
        paragraphs.append(' '.join(current_paragraph))

    return paragraphs


def extract_ending(article_content: str) -> str:
    """
    æå–æ–‡ç« ç»“å°¾

    Args:
        article_content: æ–‡ç« å†…å®¹

    Returns:
        ç»“å°¾æ–‡æœ¬
    """
    lines = article_content.split('\n')

    # å–æœ€å5-10è¡Œ
    ending_lines = lines[-10:]

    # è¿‡æ»¤ç©ºè¡Œå’Œç‰¹æ®Šæ ‡è®°
    ending_lines = [
        line.strip()
        for line in ending_lines
        if line.strip() and not line.startswith('#') and not line.startswith('>')
    ]

    return ' '.join(ending_lines[-5:])


def analyze_title_style(articles: List[Dict[str, str]]) -> Dict[str, Any]:
    """
    åˆ†ææ ‡é¢˜é£æ ¼

    Args:
        articles: æ–‡ç« åˆ—è¡¨

    Returns:
        æ ‡é¢˜é£æ ¼åˆ†æç»“æœ
    """
    titles = [extract_title(article["content"]) for article in articles]

    # é•¿åº¦ç»Ÿè®¡
    lengths = [len(title) for title in titles if title != "æœªæ‰¾åˆ°æ ‡é¢˜"]

    # æ¨¡å¼è¯†åˆ«
    patterns = {
        "æ•°å­—å¼": 0,
        "æ‚¬å¿µå¼": 0,
        "å¯¹æ¯”å¼": 0,
        "æé—®å¼": 0,
    }

    for title in titles:
        if title == "æœªæ‰¾åˆ°æ ‡é¢˜":
            continue

        # æ•°å­—å¼
        if re.search(r'\d+[ä¸ªç§æ¡é¡¹]', title):
            patterns["æ•°å­—å¼"] += 1

        # æ‚¬å¿µå¼
        if re.search(r'(æ­ç§˜|çªç ´|ç¥å™¨|é»‘ç§‘æŠ€)', title):
            patterns["æ‚¬å¿µå¼"] += 1

        # å¯¹æ¯”å¼
        if re.search(r'(çœ‹ä¼¼|å®åˆ™|ä½†æ˜¯|ç„¶è€Œ)', title):
            patterns["å¯¹æ¯”å¼"] += 1

        # æé—®å¼
        if re.search(r'[ï¼Ÿ?]$', title):
            patterns["æé—®å¼"] += 1

    # å…³é”®è¯æå–
    all_words = []
    for title in titles:
        if title == "æœªæ‰¾åˆ°æ ‡é¢˜":
            continue
        words = re.findall(r'[\w]+', title)
        all_words.extend(words)

    word_counter = Counter(all_words)
    keywords = [word for word, count in word_counter.most_common(10)]

    return {
        "patterns": patterns,
        "length": {
            "avg": sum(lengths) // len(lengths) if lengths else 0,
            "min": min(lengths) if lengths else 0,
            "max": max(lengths) if lengths else 0,
        },
        "keywords": keywords
    }


def analyze_opening_style(articles: List[Dict[str, str]]) -> Dict[str, Any]:
    """
    åˆ†æå¼€å¤´é£æ ¼

    Args:
        articles: æ–‡ç« åˆ—è¡¨

    Returns:
        å¼€å¤´é£æ ¼åˆ†æç»“æœ
    """
    openings = [extract_opening(article["content"]) for article in articles]

    # é•¿åº¦ç»Ÿè®¡
    lengths = [len(opening) for opening in openings]

    # æ¨¡å¼è¯†åˆ«
    patterns = {
        "çƒ­ç‚¹å¼•å…¥": 0,
        "ç—›ç‚¹æé—®": 0,
        "æ•°æ®éœ‡æ’¼": 0,
    }

    for opening in openings:
        # çƒ­ç‚¹å¼•å…¥
        if re.search(r'(æœ€æ–°|ä»Šå¤©|è¿‘æ—¥|æ®æŠ¥é“)', opening):
            patterns["çƒ­ç‚¹å¼•å…¥"] += 1

        # ç—›ç‚¹æé—®
        if re.search(r'[ï¼Ÿ?]', opening):
            patterns["ç—›ç‚¹æé—®"] += 1

        # æ•°æ®éœ‡æ’¼
        if re.search(r'\d+[åƒä¸‡ç™¾äº¿]', opening):
            patterns["æ•°æ®éœ‡æ’¼"] += 1

    # åŸºè°ƒåˆ†æï¼ˆç®€å•ç‰ˆï¼‰
    tone_count = {
        "ä¸“ä¸š": 0,
        "å¹½é»˜": 0,
        "çŠ€åˆ©": 0,
    }

    for opening in openings:
        # ä¸“ä¸šï¼šåŒ…å«æŠ€æœ¯æœ¯è¯­
        if re.search(r'(AI|ç®—æ³•|æ¨¡å‹|æŠ€æœ¯)', opening):
            tone_count["ä¸“ä¸š"] += 1

        # å¹½é»˜ï¼šåŒ…å«è¡¨æƒ…æˆ–è½»æ¾è¯æ±‡
        if re.search(r'(å“ˆå“ˆ|ğŸ˜€|ğŸ˜Š|æœ‰è¶£)', opening):
            tone_count["å¹½é»˜"] += 1

        # çŠ€åˆ©ï¼šåŒ…å«å¦å®šæˆ–æ‰¹åˆ¤è¯æ±‡
        if re.search(r'(ä¸|æ²¡æœ‰|ä½†æ˜¯|ç„¶è€Œ)', opening):
            tone_count["çŠ€åˆ©"] += 1

    return {
        "patterns": patterns,
        "length": {
            "avg": sum(lengths) // len(lengths) if lengths else 0,
            "min": min(lengths) if lengths else 0,
            "max": max(lengths) if lengths else 0,
        },
        "tone": max(tone_count.items(), key=lambda x: x[1])[0] if tone_count else "æœªçŸ¥"
    }


def analyze_content_structure(articles: List[Dict[str, str]]) -> Dict[str, Any]:
    """
    åˆ†æå†…å®¹ç»“æ„

    Args:
        articles: æ–‡ç« åˆ—è¡¨

    Returns:
        å†…å®¹ç»“æ„åˆ†æç»“æœ
    """
    all_paragraphs = []

    for article in articles:
        paragraphs = extract_content_body(article["content"])
        all_paragraphs.extend(paragraphs)

    # æ®µè½æ•°é‡ç»Ÿè®¡
    paragraph_counts = [len(extract_content_body(article["content"])) for article in articles]

    # æ®µè½é•¿åº¦ç»Ÿè®¡
    paragraph_lengths = [len(p) for p in all_paragraphs]

    # ç»“æ„è¯†åˆ«ï¼ˆç®€å•ç‰ˆï¼šæ€»-åˆ†-æ€»ï¼‰
    structure_count = {
        "æ€»åˆ†æ€»": 0,
        "é€’è¿›": 0,
        "å¹¶åˆ—": 0,
    }

    # ç®€å•çš„å¯å‘å¼åˆ¤æ–­
    for article in articles:
        paragraphs = extract_content_body(article["content"])
        if len(paragraphs) >= 3:
            # æ£€æŸ¥æ˜¯å¦æœ‰æ˜æ˜¾çš„æ€»ç»“æ®µè½
            first_para = paragraphs[0]
            last_para = paragraphs[-1]

            if "æ€»ç»“" in last_para or "ç»“è¯­" in last_para or "æ€»ä¹‹" in last_para:
                structure_count["æ€»åˆ†æ€»"] += 1

    return {
        "structure": max(structure_count.items(), key=lambda x: x[1])[0] if structure_count else "æœªçŸ¥",
        "paragraph_count": {
            "avg": sum(paragraph_counts) // len(paragraph_counts) if paragraph_counts else 0,
            "min": min(paragraph_counts) if paragraph_counts else 0,
            "max": max(paragraph_counts) if paragraph_counts else 0,
        },
        "paragraph_length": {
            "avg": sum(paragraph_lengths) // len(paragraph_lengths) if paragraph_lengths else 0,
            "min": min(paragraph_lengths) if paragraph_lengths else 0,
            "max": max(paragraph_lengths) if paragraph_lengths else 0,
        }
    }


def analyze_language_style(articles: List[Dict[str, str]]) -> Dict[str, Any]:
    """
    åˆ†æè¯­è¨€é£æ ¼

    Args:
        articles: æ–‡ç« åˆ—è¡¨

    Returns:
        è¯­è¨€é£æ ¼åˆ†æç»“æœ
    """
    all_text = ' '.join([article["content"] for article in articles])

    # å¥é•¿ç»Ÿè®¡ï¼ˆç®€å•ç‰ˆï¼šæŒ‰æ ‡ç‚¹åˆ†å‰²ï¼‰
    sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]', all_text)
    sentences = [s.strip() for s in sentences if s.strip()]
    sentence_lengths = [len(s) for s in sentences]

    # è¯æ±‡å¤šæ ·æ€§
    words = re.findall(r'[\w]+', all_text)
    word_counter = Counter(words)
    vocabulary_diversity = len(word_counter) / len(words) if words else 0

    # åŸºè°ƒåˆ†æ
    tone_keywords = {
        "ä¸“ä¸š": ["æŠ€æœ¯", "ç®—æ³•", "æ¨¡å‹", "ç³»ç»Ÿ", "æ¶æ„"],
        "å¹½é»˜": ["å“ˆå“ˆ", "ğŸ˜€", "ğŸ˜Š", "æœ‰è¶£", "å¥½ç©"],
        "çŠ€åˆ©": ["ä¸", "æ²¡æœ‰", "ä½†æ˜¯", "ç„¶è€Œ", "é—®é¢˜"],
    }

    tone_scores = {tone: 0 for tone in tone_keywords}
    for tone, keywords in tone_keywords.items():
        for keyword in keywords:
            tone_scores[tone] += all_text.count(keyword)

    return {
        "vocabulary": f"ä¸“ä¸šæœ¯è¯­+é€šä¿—è§£é‡Š" if tone_scores["ä¸“ä¸š"] > 10 else "é€šä¿—ä¸ºä¸»",
        "sentence_length": {
            "avg": sum(sentence_lengths) // len(sentence_lengths) if sentence_lengths else 0,
            "min": min(sentence_lengths) if sentence_lengths else 0,
            "max": max(sentence_lengths) if sentence_lengths else 0,
        },
        "tone": max(tone_scores.items(), key=lambda x: x[1])[0] if tone_scores else "æœªçŸ¥",
        "vocabulary_diversity": round(vocabulary_diversity, 2)
    }


def analyze_ending_style(articles: List[Dict[str, str]]) -> Dict[str, Any]:
    """
    åˆ†æç»“å°¾é£æ ¼

    Args:
        articles: æ–‡ç« åˆ—è¡¨

    Returns:
        ç»“å°¾é£æ ¼åˆ†æç»“æœ
    """
    endings = [extract_ending(article["content"]) for article in articles]

    # é•¿åº¦ç»Ÿè®¡
    lengths = [len(ending) for ending in endings]

    # æ¨¡å¼è¯†åˆ«
    patterns = {
        "æ€»ç»“æå‡": 0,
        "è¡ŒåŠ¨å·å¬": 0,
        "ç¦åˆ©å¼•å¯¼": 0,
    }

    for ending in endings:
        # æ€»ç»“æå‡
        if re.search(r'(æ€»ç»“|æ€»è€Œè¨€ä¹‹|æ€»ä¹‹|ç»¼ä¸Š)', ending):
            patterns["æ€»ç»“æå‡"] += 1

        # è¡ŒåŠ¨å·å¬
        if re.search(r'(å…³æ³¨|ç‚¹èµ|æ”¶è—|è½¬å‘|åˆ†äº«)', ending):
            patterns["è¡ŒåŠ¨å·å¬"] += 1

        # ç¦åˆ©å¼•å¯¼
        if re.search(r'(è·å–|ä¸‹è½½|é¢†å–|å…è´¹|ç¦åˆ©)', ending):
            patterns["ç¦åˆ©å¼•å¯¼"] += 1

    return {
        "patterns": patterns,
        "length": {
            "avg": sum(lengths) // len(lengths) if lengths else 0,
            "min": min(lengths) if lengths else 0,
            "max": max(lengths) if lengths else 0,
        },
        "call_to_action": max(patterns.items(), key=lambda x: x[1])[0] if patterns else "æ— "
    }


def analyze_tone_style(articles: List[Dict[str, str]]) -> Dict[str, Any]:
    """
    åˆ†æè¯­æ°”/è¯­ä½“è‰²å½©

    Args:
        articles: æ–‡ç« åˆ—è¡¨

    Returns:
        è¯­æ°”é£æ ¼åˆ†æç»“æœ
    """
    all_text = ' '.join([article["content"] for article in articles])

    # è¯­æ°”ç»´åº¦åˆ†æ
    tone_keywords = {
        "è±ªæ”¾": ["é›„å¿ƒ", "ä¼Ÿä¸š", "å¼€é˜”", "æ¢å¼˜", "ç£…ç¤´", "æ¿€æ˜‚", "æ…·æ…¨"],
        "æŸ”å©‰": ["çº¤å·§", "ç»†è‡´", "ç¼ ç»µ", "æŸ”", "å©‰", "ç»†è…»"],
        "ç›´éœ²": ["ç›´æ¥", "æ˜ç¡®", "æ˜¾ç„¶", "æ˜¾ç„¶", "ç›´è¨€"],
        "å«è“„": ["å«è“„", "å§”å©‰", "æš—ç¤º", "å¯“æ„", "éšå–»"],
        "å¹½é»˜": ["å“ˆå“ˆ", "ğŸ˜€", "ğŸ˜Š", "æœ‰è¶£", "å¥½ç©", "å¹½é»˜", "æç¬‘"],
        "æ²‰éƒ": ["æ²‰éƒ", "å‡„å‡‰", "æ‚²ä¼¤", "å“€æ„", "å¿§éƒ"],
        "æ¸…æ–°": ["æ¸…æ–°", "æ˜å¿«", "æ˜æœ—", "è½»å¿«"],
        "åä¸½": ["åä¸½", "å…¸é›…", "ç‘°ä¸½", "ç»®ä¸½", "ç»šçƒ‚"],
        "ç´ æœ´": ["ç´ æœ´", "æœ´ç´ ", "æœ´å®", "è‡ªç„¶", "æ·¡é›…"],
    }

    tone_scores = {tone: 0 for tone in tone_keywords}
    for tone, keywords in tone_keywords.items():
        for keyword in keywords:
            tone_scores[tone] += all_text.count(keyword)

    # åˆ¤æ–­ä¸»è¦è¯­æ°”
    dominant_tone = max(tone_scores.items(), key=lambda x: x[1])[0] if tone_scores else "ä¸­æ€§"

    # è¯­æ°”å¼ºåº¦
    tone_intensity = "å¼º" if tone_scores[dominant_tone] > 10 else "å¼±"

    return {
        "dominant_tone": dominant_tone,
        "tone_intensity": tone_intensity,
        "tone_scores": tone_scores
    }


def analyze_emotion_style(articles: List[Dict[str, str]]) -> Dict[str, Any]:
    """
    åˆ†ææƒ…æ„Ÿè‰²å½©

    Args:
        articles: æ–‡ç« åˆ—è¡¨

    Returns:
        æƒ…æ„Ÿé£æ ¼åˆ†æç»“æœ
    """
    all_text = ' '.join([article["content"] for article in articles])

    # æƒ…æ„Ÿè¯æ±‡
    emotion_keywords = {
        "æ­£é¢": ["å¥½", "ä¼˜ç§€", "æ£’", "èµ", "å–œæ¬¢", "çˆ±", "æˆåŠŸ", "ä¼˜ç§€", "ç²¾å½©"],
        "è´Ÿé¢": ["ä¸å¥½", "å·®", "å", "è®¨åŒ", "æ¨", "å¤±è´¥", "ç³Ÿç³•", "å·®åŠ²"],
        "æ¿€è¶Š": ["æ¿€æ˜‚", "æ…·æ…¨", "æ¿€è¶Š", "çƒ­æƒ…", "çƒ­çƒˆ"],
        "æ˜å¿«": ["æ˜å¿«", "æ˜äº®", "æ¬¢å¿«", "å¿«ä¹", "å–œæ‚¦"],
        "æ²‰éƒ": ["æ²‰éƒ", "å¿§ä¼¤", "æ‚²ä¼¤", "å“€æ„"],
        "å«è“„": ["å«è“„", "å§”å©‰", "æ·±æ²‰"],
    }

    emotion_scores = {emotion: 0 for emotion in emotion_keywords}
    for emotion, keywords in emotion_keywords.items():
        for keyword in keywords:
            emotion_scores[emotion] += all_text.count(keyword)

    # æƒ…æ„Ÿå€¾å‘
    positive_score = emotion_scores["æ­£é¢"] + emotion_scores["æ˜å¿«"]
    negative_score = emotion_scores["è´Ÿé¢"] + emotion_scores["æ²‰éƒ"]
    total_score = positive_score + negative_score

    if total_score == 0:
        sentiment_trend = "ä¸­æ€§"
    elif positive_score > negative_score:
        sentiment_trend = "æ­£é¢"
    else:
        sentiment_trend = "è´Ÿé¢"

    # æƒ…æ„Ÿå¼ºåº¦
    emotion_intensity = "å¼º" if total_score > 20 else "ä¸­" if total_score > 10 else "å¼±"

    # ä¸»å¯¼æƒ…æ„Ÿ
    dominant_emotion = max(emotion_scores.items(), key=lambda x: x[1])[0] if emotion_scores else "ä¸­æ€§"

    return {
        "sentiment_trend": sentiment_trend,
        "emotion_intensity": emotion_intensity,
        "dominant_emotion": dominant_emotion,
        "emotion_scores": emotion_scores
    }


def analyze_common_phrases(articles: List[Dict[str, str]]) -> Dict[str, Any]:
    """
    åˆ†æå¸¸ç”¨è¯­é£æ ¼

    Args:
        articles: æ–‡ç« åˆ—è¡¨

    Returns:
        å¸¸ç”¨è¯­é£æ ¼åˆ†æç»“æœ
    """
    all_text = ' '.join([article["content"] for article in articles])

    # å¸¸ç”¨è¯­ç±»å‹
    phrase_patterns = {
        "ä¹¦é¢è¯­": ["å› æ­¤", "å› è€Œ", "ç”±æ­¤å¯è§", "ç»¼ä¸Šæ‰€è¿°", "æ€»è€Œè¨€ä¹‹"],
        "å£å¤´è¯­": ["å§", "å‘¢", "å•Š", "å—", "å˜›", "å“ˆ"],
        "å è¯": [],  # åŠ¨æ€æå–
        "æˆè¯­": [],  # åŠ¨æ€æå–
        "ç½‘ç»œç”¨è¯­": ["æ‰“å¡", "ç§è‰", "æ‹”è‰", "åƒç“œ", "èººå¹³", "å†…å·"],
        "ä¸“ä¸šæœ¯è¯­": ["AI", "ç®—æ³•", "æ¨¡å‹", "æ•°æ®", "åˆ†æ", "æŠ€æœ¯"],
    }

    phrase_counts = {phrase: 0 for phrase in phrase_patterns}

    # ç»Ÿè®¡å›ºå®šçŸ­è¯­
    for phrase, keywords in phrase_patterns.items():
        if keywords:  # éç©º
            for keyword in keywords:
                phrase_counts[phrase] += all_text.count(keyword)

    # æå–å è¯ï¼ˆæ¨¡å¼ï¼šAA, ABB, AABBï¼‰
    reduplicated_words = re.findall(r'(.)\1+(?=.{0,2})', all_text)
    phrase_counts["å è¯"] = len(reduplicated_words)

    # æå–é«˜é¢‘çŸ­è¯­ï¼ˆ3-4å­—ï¼‰
    words = re.findall(r'[\u4e00-\u9fa5]{3,4}', all_text)
    word_counter = Counter(words)
    common_phrases = word_counter.most_common(20)

    # åˆ¤æ–­ä¸»è¦ç”¨è¯­é£æ ¼
    dominant_phrase_style = max(phrase_counts.items(), key=lambda x: x[1])[0] if phrase_counts else "æ··åˆ"

    return {
        "dominant_style": dominant_phrase_style,
        "phrase_counts": phrase_counts,
        "top_common_phrases": common_phrases[:10],
        "reduplicated_word_count": len(reduplicated_words)
    }


def analyze_rhetorical_devices(articles: List[Dict[str, str]]) -> Dict[str, Any]:
    """
    åˆ†æä¿®è¾æ‰‹æ³•

    Args:
        articles: æ–‡ç« åˆ—è¡¨

    Returns:
        ä¿®è¾æ‰‹æ³•åˆ†æç»“æœ
    """
    all_text = ' '.join([article["content"] for article in articles])

    # ä¿®è¾æ‰‹æ³•æ£€æµ‹ï¼ˆç®€åŒ–ç‰ˆï¼‰
    rhetorical_devices = {
        "æ¯”å–»": 0,  # å«"åƒ"ã€"å¦‚"ã€"ä¼¼"
        "æ‹Ÿäºº": 0,  # å«æ‹ŸäººåŠ¨è¯
        "å¤¸å¼ ": 0,  # å«å¤¸å¼ è¯
        "æ’æ¯”": 0,  # æ£€æµ‹æ’æ¯”å¥å¼
        "è®¾é—®": 0,  # å«"ï¼Ÿ"
        "åé—®": 0,  # æ£€æµ‹åé—®å¥å¼
        "å¼•ç”¨": 0,  # å«"å¼•å·"æˆ–ä¹¦åå·
        "å¯¹å¶": 0,  # æ£€æµ‹å¯¹å¶å¥å¼
    }

    # æ¯”å–»
    if re.search(r'(åƒ|å¦‚|ä¼¼|ä»¿ä½›|å¥½æ¯”)', all_text):
        rhetorical_devices["æ¯”å–»"] += all_text.count("åƒ") + all_text.count("å¦‚") + all_text.count("ä¼¼")

    # æ‹Ÿäººï¼ˆç®€åŒ–ç‰ˆï¼šæ£€æµ‹å¸¸è§æ‹Ÿäººè¯ï¼‰
    personification_words = ["å¾®ç¬‘", "è·³èˆ", "æ­Œå”±", "å“­æ³£", "æ€’å¼"]
    for word in personification_words:
        rhetorical_devices["æ‹Ÿäºº"] += all_text.count(word)

    # å¤¸å¼ ï¼ˆç®€åŒ–ç‰ˆï¼šæ£€æµ‹å¤¸å¼ è¯ï¼‰
    exaggeration_words = ["æå…¶", "éå¸¸", "è¶…çº§", "åƒä¸‡", "æ— æ•°"]
    for word in exaggeration_words:
        rhetorical_devices["å¤¸å¼ "] += all_text.count(word)

    # æ’æ¯”ï¼ˆç®€åŒ–ç‰ˆï¼šæ£€æµ‹é‡å¤ç»“æ„ï¼‰
    if re.search(r'(.{5,20})[ï¼Œã€‚].*\1[ï¼Œã€‚]', all_text):
        rhetorical_devices["æ’æ¯”"] += 1

    # è®¾é—®/åé—®
    rhetorical_devices["è®¾é—®"] = all_text.count("ï¼Ÿ")
    if re.search(r'(éš¾é“|å²‚ä¸æ˜¯|æ€ä¹ˆèƒ½)', all_text):
        rhetorical_devices["åé—®"] += 1

    # å¼•ç”¨
    rhetorical_devices["å¼•ç”¨"] = all_text.count('"') // 2 + all_text.count('"') // 2

    # å¯¹å¶ï¼ˆç®€åŒ–ç‰ˆï¼šæ£€æµ‹å¯¹å¶ç»“æ„ï¼‰
    if re.search(r'(.{4,10})[ï¼Œã€‚].*(.{4,10})[ï¼Œã€‚]', all_text):
        rhetorical_devices["å¯¹å¶"] += 1

    # è®¡ç®—æ€»æ•°
    total_devices = sum(rhetorical_devices.values())

    # ä¸»è¦ä¿®è¾æ‰‹æ³•
    dominant_device = max(rhetorical_devices.items(), key=lambda x: x[1])[0] if rhetorical_devices else "æ— æ˜æ˜¾ä¿®è¾"

    return {
        "dominant_device": dominant_device,
        "device_counts": rhetorical_devices,
        "total_devices": total_devices,
        "device_density": round(total_devices / len(all_text) * 100, 2) if all_text else 0
    }


def analyze_sentence_structure(articles: List[Dict[str, str]]) -> Dict[str, Any]:
    """
    åˆ†æå¥å¼é£æ ¼

    Args:
        articles: æ–‡ç« åˆ—è¡¨

    Returns:
        å¥å¼é£æ ¼åˆ†æç»“æœ
    """
    all_text = ' '.join([article["content"] for article in articles])

    # å¥å­åˆ†å‰²
    sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]', all_text)
    sentences = [s.strip() for s in sentences if s.strip()]
    sentence_lengths = [len(s) for s in sentences]

    # å¥å¼ç±»å‹
    sentence_types = {
        "æ•´å¥": 0,  # ç»“æ„å®Œæ•´
        "æ•£å¥": 0,  # ç»“æ„ä¸å®Œæ•´
        "é•¿å¥": 0,  # >30å­—
        "çŸ­å¥": 0,  # <15å­—
        "æ„Ÿå¹å¥": 0,  # å«"ï¼"
        "ç–‘é—®å¥": 0,  # å«"ï¼Ÿ"
    }

    for sentence in sentences:
        length = len(sentence)

        # åˆ¤æ–­æ•´å¥/æ•£å¥ï¼ˆç®€åŒ–ç‰ˆï¼šæ ¹æ®æ ‡ç‚¹ï¼‰
        if sentence and (sentence[-1] in ['ã€‚', 'ï¼', 'ï¼Ÿ', 'ã€‚', 'ï¼', 'ï¼Ÿ']):
            sentence_types["æ•´å¥"] += 1
        else:
            sentence_types["æ•£å¥"] += 1

        # åˆ¤æ–­é•¿çŸ­
        if length > 30:
            sentence_types["é•¿å¥"] += 1
        elif length < 15:
            sentence_types["çŸ­å¥"] += 1

        # æ„Ÿå¹å¥
        if 'ï¼' in sentence or '!' in sentence:
            sentence_types["æ„Ÿå¹å¥"] += 1

        # ç–‘é—®å¥
        if 'ï¼Ÿ' in sentence or '?' in sentence:
            sentence_types["ç–‘é—®å¥"] += 1

    # æ•´æ•£å¥æ¯”ä¾‹
    total_sentences = len(sentences)
    if total_sentences > 0:
        ratio_zhengsan = sentence_types["æ•´å¥"] / total_sentences
    else:
        ratio_zhengsan = 0.5

    # é•¿çŸ­å¥æ¯”ä¾‹
    if total_sentences > 0:
        ratio_changduan = sentence_types["é•¿å¥"] / (sentence_types["çŸ­å¥"] + 1)
    else:
        ratio_changduan = 0.5

    # ä¸»è¦å¥å¼ç±»å‹
    if ratio_zhengsan > 0.7:
        dominant_type = "ä»¥æ•´å¥ä¸ºä¸»"
    elif ratio_zhengsan < 0.3:
        dominant_type = "ä»¥æ•£å¥ä¸ºä¸»"
    else:
        dominant_type = "æ•´æ•£ç»“åˆ"

    return {
        "dominant_type": dominant_type,
        "sentence_types": sentence_types,
        "sentence_length": {
            "avg": sum(sentence_lengths) // len(sentence_lengths) if sentence_lengths else 0,
            "min": min(sentence_lengths) if sentence_lengths else 0,
            "max": max(sentence_lengths) if sentence_lengths else 0,
        },
        "ratio_zhengsan": round(ratio_zhengsan, 2),
        "ratio_changduan": round(ratio_changduan, 2)
    }


def analyze_style(articles_text: str) -> Dict[str, Any]:
    """
    åˆ†æå†™ä½œé£æ ¼ï¼ˆä¸»å‡½æ•°ï¼‰

    Args:
        articles_text: æ–‡ç« æ–‡æœ¬

    Returns:
        é£æ ¼åˆ†æç»“æœ
    """
    # è§£ææ–‡ç« 
    articles = parse_articles(articles_text)

    if len(articles) < 10:
        print(f"âš ï¸ è­¦å‘Šï¼šåªæœ‰{len(articles)}ç¯‡æ–‡ç« ï¼Œå»ºè®®è‡³å°‘10ç¯‡æ–‡ç« ")

    # åˆ†æå„ä¸ªç»´åº¦
    title_style = analyze_title_style(articles)
    opening_style = analyze_opening_style(articles)
    content_structure = analyze_content_structure(articles)
    language_style = analyze_language_style(articles)
    ending_style = analyze_ending_style(articles)

    # æ–°å¢ï¼šåˆ†ææ›´å¤šç»´åº¦
    tone_style = analyze_tone_style(articles)
    emotion_style = analyze_emotion_style(articles)
    common_phrases_style = analyze_common_phrases(articles)
    rhetorical_devices_style = analyze_rhetorical_devices(articles)
    sentence_structure_style = analyze_sentence_structure(articles)

    # ç”Ÿæˆé£æ ¼æè¿°
    style_description = f"åŸºäº{len(articles)}ç¯‡æ–‡ç« çš„åˆ†æï¼Œ"

    # ç»¼åˆé£æ ¼ç‰¹å¾
    if title_style["patterns"]["æ•°å­—å¼"] > 0:
        style_description += "æ ‡é¢˜å€¾å‘äºä½¿ç”¨æ•°å­—å¼è¡¨è¾¾ï¼Œ"

    if opening_style["tone"] == "ä¸“ä¸š":
        style_description += "å¼€å¤´é£æ ¼ä¸“ä¸šï¼Œ"

    if content_structure["structure"] == "æ€»åˆ†æ€»":
        style_description += "å†…å®¹ç»“æ„é‡‡ç”¨æ€»åˆ†æ€»ï¼Œ"

    if language_style["tone"] == "ä¸“ä¸š":
        style_description += "è¯­è¨€é£æ ¼åä¸“ä¸šï¼Œ"

    style_description += "å†…å®¹å®ç”¨æ€§è¾ƒå¼ºã€‚"

    # ç”Ÿæˆé£æ ¼æ ‡ç­¾
    style_tags = []

    if title_style["patterns"]["æ•°å­—å¼"] > 0:
        style_tags.append("æ•°å­—å¼")

    if title_style["patterns"]["æ‚¬å¿µå¼"] > 0:
        style_tags.append("æ‚¬å¿µå¼")

    if title_style["patterns"]["å¯¹æ¯”å¼"] > 0:
        style_tags.append("å¯¹æ¯”å¼")

    if language_style["tone"] == "ä¸“ä¸š":
        style_tags.append("ä¸“ä¸š")

    if len(articles) >= 10:
        style_tags.append("å¹²è´§")

    # è®¡ç®—é£æ ¼è¯„åˆ†ï¼ˆç®€å•ç‰ˆï¼‰
    style_score = 70  # åŸºç¡€åˆ†
    if len(articles) >= 10:
        style_score += 10
    if len(articles) >= 20:
        style_score += 10
    if title_style["patterns"]["æ•°å­—å¼"] > 0:
        style_score += 5

    return {
        "status": "success",
        "article_count": len(articles),
        "style_features": {
            "title_style": title_style,
            "opening_style": opening_style,
            "content_structure": content_structure,
            "language_style": language_style,
            "ending_style": ending_style,
            "tone_style": tone_style,
            "emotion_style": emotion_style,
            "common_phrases_style": common_phrases_style,
            "rhetorical_devices_style": rhetorical_devices_style,
            "sentence_structure_style": sentence_structure_style
        },
        "style_description": style_description,
        "style_tags": style_tags,
        "style_score": style_score
    }


def generate_style_prompt(style_analysis: Dict[str, Any]) -> str:
    """
    ç”Ÿæˆé£æ ¼Prompt

    Args:
        style_analysis: é£æ ¼åˆ†æç»“æœ

    Returns:
        é£æ ¼Prompt
    """
    features = style_analysis["style_features"]
    description = style_analysis["style_description"]
    tags = style_analysis["style_tags"]

    prompt = f"""# å†™ä½œé£æ ¼æŒ‡å—

## åŸºæœ¬å®šä½
{description}

## é£æ ¼æ ‡ç­¾
{', '.join(tags)}

## æ ‡é¢˜é£æ ¼

### å¸¸ç”¨æ¨¡å¼
"""

    # æ ‡é¢˜æ¨¡å¼
    title_patterns = features["title_style"]["patterns"]
    for pattern, count in title_patterns.items():
        if count > 0:
            prompt += f"- {pattern}ï¼ˆ{count}ç¯‡ä½¿ç”¨ï¼‰\n"

    prompt += f"""

### æ ‡é¢˜ç‰¹å¾
- é•¿åº¦ï¼š{features["title_style"]["length"]["min"]}-{features["title_style"]["length"]["max"]}å­—
- å¹³å‡é•¿åº¦ï¼š{features["title_style"]["length"]["avg"]}å­—
- å…³é”®è¯ï¼š{', '.join(features["title_style"]["keywords"][:5])}

## å¼€å¤´é£æ ¼

### å¸¸ç”¨æ¨¡å¼
"""

    # å¼€å¤´æ¨¡å¼
    opening_patterns = features["opening_style"]["patterns"]
    for pattern, count in opening_patterns.items():
        if count > 0:
            prompt += f"- {pattern}ï¼ˆ{count}ç¯‡ä½¿ç”¨ï¼‰\n"

    prompt += f"""

### å¼€å¤´ç‰¹å¾
- é•¿åº¦ï¼š{features["opening_style"]["length"]["min"]}-{features["opening_style"]["length"]["max"]}å­—
- å¹³å‡é•¿åº¦ï¼š{features["opening_style"]["length"]["avg"]}å­—
- åŸºè°ƒï¼š{features["opening_style"]["tone"]}

## å†…å®¹ç»“æ„

### æ•´ä½“ç»“æ„
{features["content_structure"]["structure"]}

### æ®µè½ç»„ç»‡
- æ®µè½æ•°é‡ï¼šå¹³å‡{features["content_structure"]["paragraph_count"]["avg"]}æ®µï¼ˆèŒƒå›´ï¼š{features["content_structure"]["paragraph_count"]["min"]}-{features["content_structure"]["paragraph_count"]["max"]}æ®µï¼‰
- æ®µè½é•¿åº¦ï¼šå¹³å‡{features["content_structure"]["paragraph_length"]["avg"]}å­—ï¼ˆèŒƒå›´ï¼š{features["content_structure"]["paragraph_length"]["min"]}-{features["content_structure"]["paragraph_length"]["max"]}å­—ï¼‰

## è¯­è¨€é£æ ¼

### è¯æ±‡é€‰æ‹©
{features["language_style"]["vocabulary"]}

### å¥å¼ç‰¹ç‚¹
- å¥é•¿ï¼šå¹³å‡{features["language_style"]["sentence_length"]["avg"]}å­—ï¼ˆèŒƒå›´ï¼š{features["language_style"]["sentence_length"]["min"]}-{features["language_style"]["sentence_length"]["max"]}å­—ï¼‰
- åŸºè°ƒï¼š{features["language_style"]["tone"]}
- è¯æ±‡å¤šæ ·æ€§ï¼š{features["language_style"]["vocabulary_diversity"]}

## è¯­æ°”é£æ ¼

### ä¸»å¯¼è¯­æ°”
- è¯­æ°”ç±»å‹ï¼š{features["tone_style"]["dominant_tone"]}
- è¯­æ°”å¼ºåº¦ï¼š{features["tone_style"]["tone_intensity"]}

### è¯­æ°”ç‰¹ç‚¹
{', '.join([f'- {{tone}}: {{count}}æ¬¡' for tone, count in features["tone_style"]["tone_scores"].items() if count > 0]) if any(features["tone_style"]["tone_scores"].values()) else '- è¯­æ°”åˆ†å¸ƒå‡åŒ€'}

## æƒ…æ„Ÿè‰²å½©

### æƒ…æ„Ÿå€¾å‘
- æƒ…æ„Ÿå€¾å‘ï¼š{features["emotion_style"]["sentiment_trend"]}
- æƒ…æ„Ÿå¼ºåº¦ï¼š{features["emotion_style"]["emotion_intensity"]}
- ä¸»å¯¼æƒ…æ„Ÿï¼š{features["emotion_style"]["dominant_emotion"]}

### æƒ…æ„Ÿåˆ†å¸ƒ
{', '.join([f'- {{emotion}}: {{count}}æ¬¡' for emotion, count in features["emotion_style"]["emotion_scores"].items() if count > 0]) if any(features["emotion_style"]["emotion_scores"].values()) else '- æƒ…æ„Ÿåˆ†å¸ƒå‡åŒ€'}

## å¸¸ç”¨è¯­é£æ ¼

### ä¸»å¯¼ç”¨è¯­é£æ ¼
- ç”¨è¯­é£æ ¼ï¼š{features["common_phrases_style"]["dominant_style"]}

### ç”¨è¯­ç‰¹ç‚¹
- å è¯æ•°é‡ï¼š{features["common_phrases_style"]["reduplicated_word_count"]}ä¸ª
- é«˜é¢‘çŸ­è¯­ï¼š{', '.join([phrase for phrase, count in features["common_phrases_style"]["top_common_phrases"][:5]])}

## ä¿®è¾æ‰‹æ³•

### ä¸»è¦ä¿®è¾æ‰‹æ³•
- ä¸»å¯¼æ‰‹æ³•ï¼š{features["rhetorical_devices_style"]["dominant_device"]}
- ä¿®è¾å¯†åº¦ï¼š{features["rhetorical_devices_style"]["device_density"]}%

### ä¿®è¾åˆ†å¸ƒ
{', '.join([f'- {{device}}: {{count}}æ¬¡' for device, count in features["rhetorical_devices_style"]["device_counts"].items() if count > 0]) if any(features["rhetorical_devices_style"]["device_counts"].values()) else '- æ— æ˜æ˜¾ä¿®è¾'}

## å¥å¼é£æ ¼

### å¥å¼ç‰¹ç‚¹
- ä¸»è¦å¥å¼ï¼š{features["sentence_structure_style"]["dominant_type"]}
- æ•´æ•£å¥æ¯”ä¾‹ï¼š{features["sentence_structure_style"]["ratio_zhengsan"]}

### å¥å¼åˆ†å¸ƒ
{', '.join([f'- {{stype}}: {{count}}å¥' for stype, count in features["sentence_structure_style"]["sentence_types"].items() if count > 0]) if any(features["sentence_structure_style"]["sentence_types"].values()) else '- å¥å¼åˆ†å¸ƒå‡åŒ€'}
- å¥é•¿ï¼šå¹³å‡{features["sentence_structure_style"]["sentence_length"]["avg"]}å­—ï¼ˆèŒƒå›´ï¼š{features["sentence_structure_style"]["sentence_length"]["min"]}-{features["sentence_structure_style"]["sentence_length"]["max"]}å­—ï¼‰

## ç»“å°¾é£æ ¼

### å¸¸ç”¨æ¨¡å¼
"""

    # ç»“å°¾æ¨¡å¼
    ending_patterns = features["ending_style"]["patterns"]
    for pattern, count in ending_patterns.items():
        if count > 0:
            prompt += f"- {pattern}ï¼ˆ{count}ç¯‡ä½¿ç”¨ï¼‰\n"

    prompt += f"""

### ç»“å°¾ç‰¹å¾
- é•¿åº¦ï¼š{features["ending_style"]["length"]["min"]}-{features["ending_style"]["length"]["max"]}å­—
- å¹³å‡é•¿åº¦ï¼š{features["ending_style"]["length"]["avg"]}å­—
- è¡ŒåŠ¨å·å¬ï¼š{features["ending_style"]["call_to_action"]}

## å†™ä½œå»ºè®®

åŸºäºåˆ†æç»“æœï¼Œå»ºè®®ï¼š
1. ä¿æŒ{features["tone_style"]["dominant_tone"]}çš„è¯­æ°”
2. ä¿æŒ{features["emotion_style"]["dominant_emotion"]}çš„æƒ…æ„ŸåŸºè°ƒ
3. ç»§ç»­ä½¿ç”¨æ•°å­—å¼å¼æ ‡é¢˜
4. é‡‡ç”¨{features["content_structure"]["structure"]}ç»“æ„
5. æ³¨æ„æ®µè½é•¿åº¦æ§åˆ¶
6. é€‚å½“ä½¿ç”¨{features["rhetorical_devices_style"]["dominant_device"]}ç­‰ä¿®è¾æ‰‹æ³•
7. ä¿æŒ{features["common_phrases_style"]["dominant_style"]}çš„ç”¨è¯­é£æ ¼
8. æ³¨æ„{features["sentence_structure_style"]["dominant_type"]}çš„å¥å¼ç‰¹ç‚¹
"""

    return prompt


def generate_titles(style_analysis: Dict[str, Any], topic: str, limit: int = 10) -> List[str]:
    """
    åŸºäºé£æ ¼ç”Ÿæˆæ ‡é¢˜

    Args:
        style_analysis: é£æ ¼åˆ†æç»“æœ
        topic: ä¸»é¢˜
        limit: ç”Ÿæˆæ•°é‡

    Returns:
        æ ‡é¢˜åˆ—è¡¨
    """
    title_patterns = style_analysis["style_features"]["title_style"]["patterns"]
    keywords = style_analysis["style_features"]["title_style"]["keywords"]

    titles = []

    # åŸºäºä¸åŒæ¨¡å¼ç”Ÿæˆæ ‡é¢˜
    # æ•°å­—å¼
    if title_patterns["æ•°å­—å¼"] > 0:
        titles.append(f"5å¤§äº®ç‚¹æ­ç§˜ï¼{topic}å®æˆ˜æŒ‡å—")
        titles.append(f"æŒæ¡è¿™6ä¸ª{topic}æŠ€å·§ï¼Œæ•ˆç‡æå‡10å€")

    # æ‚¬å¿µå¼
    if title_patterns["æ‚¬å¿µå¼"] > 0:
        titles.append(f"{topic}æ·±åº¦è§£æï¼šæ­ç§˜èƒŒåçœŸç›¸")
        titles.append(f"{topic}å¤§çªç ´ï¼šæ•ˆç‡æå‡çš„ç§˜å¯†")

    # å¯¹æ¯”å¼
    if title_patterns["å¯¹æ¯”å¼"] > 0:
        titles.append(f"çœ‹ä¼¼ç®€å•ï¼Œå®åˆ™å¼ºå¤§ï¼{topic}å®æˆ˜æµ‹è¯„")

    # æé—®å¼
    if title_patterns["æé—®å¼"] > 0:
        titles.append(f"ä¸ºä»€ä¹ˆ{topic}è¿™ä¹ˆå—æ¬¢è¿ï¼Ÿ")

    # è¡¥å……é€šç”¨æ ‡é¢˜
    titles.append(f"{topic}å®Œå…¨æŒ‡å—")
    titles.append(f"{topic}æœ€ä½³å®è·µ")

    # åŸºäºå…³é”®è¯ç”Ÿæˆ
    if keywords:
        titles.append(f"{keywords[0]}ï¼š{topic}æ–°çªç ´")

    return titles[:limit]


def save_to_file(data: Any, output_file: str) -> bool:
    """
    ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶

    Args:
        data: è¦ä¿å­˜çš„æ•°æ®
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„

    Returns:
        æ˜¯å¦æˆåŠŸ
    """
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            if isinstance(data, str):
                f.write(data)
            else:
                json.dump(data, f, ensure_ascii=False, indent=2)
        return True
    except Exception as e:
        print(f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
        return False




# ===== èˆ¹é•¿å¼é£æ ¼åˆ†æï¼ˆ2026-01-15æ–°å¢ï¼‰ =====

def analyze_captain_style(articles_text: str) -> Dict[str, Any]:
    """
    åˆ†æèˆ¹é•¿å¼å†™ä½œé£æ ¼

    åŸºäºèˆ¹é•¿AIè§†ç•Œ100ç¯‡æ–‡ç« åˆ†æï¼Œæå–èˆ¹é•¿ç‰¹æœ‰çš„å†™ä½œé£æ ¼ç‰¹å¾ï¼š
    1. å¼€å¤´ç­–ç•¥ï¼šçƒ­ç‚¹å¼•å…¥+ç—›ç‚¹+ç¨€ç¼ºæ€§
    2. å†…å®¹ç»“æ„ï¼šå››æ®µå¼ï¼ˆä»‹ç»â†’å¯¹æ¯”â†’ä½“éªŒâ†’å»ºè®®ï¼‰
    3. æ•°æ®æ”¯æ’‘ï¼šæ¡ˆä¾‹å¯†åº¦+æ•°æ®å¼•ç”¨
    4. è¯­è¨€é£æ ¼ï¼šçœŸè¯š+æ¥åœ°æ°”+é€‚åº¦æƒ…ç»ªè¯
    5. ç»“å°¾è®¾è®¡ï¼šäº’åŠ¨å¼•å¯¼+ç§åŸŸè½¬åŒ–

    Args:
        articles_text: æ–‡ç« æ–‡æœ¬

    Returns:
        èˆ¹é•¿å¼é£æ ¼åˆ†æç»“æœ
    """
    articles = parse_articles(articles_text)

    # 1. åˆ†æå¼€å¤´ç­–ç•¥
    opening_strategy = _analyze_captain_opening(articles)

    # 2. åˆ†æå†…å®¹ç»“æ„
    content_structure = _analyze_captain_structure(articles)

    # 3. åˆ†ææ•°æ®æ”¯æ’‘
    data_support = _analyze_captain_data_support(articles)

    # 4. åˆ†æè¯­è¨€é£æ ¼
    language_style = _analyze_captain_language(articles)

    # 5. åˆ†æç»“å°¾è®¾è®¡
    ending_design = _analyze_captain_ending(articles)

    # è®¡ç®—èˆ¹é•¿é£æ ¼æ€»åˆ†
    captain_score = (
        opening_strategy["score"] * 0.2 +
        content_structure["score"] * 0.25 +
        data_support["score"] * 0.2 +
        language_style["score"] * 0.2 +
        ending_design["score"] * 0.15
    )

    # åˆ¤æ–­ç­‰çº§
    if captain_score >= 80:
        grade = "ä¼˜ç§€ï¼ˆèˆ¹é•¿çº§åˆ«ï¼‰"
        advice = "è¿™ä¸ªé£æ ¼å·²ç»è¾¾åˆ°äº†èˆ¹é•¿çš„çˆ†æ¬¾æ ‡å‡†ï¼"
    elif captain_score >= 60:
        grade = "è‰¯å¥½"
        advice = "é£æ ¼ä¸é”™ï¼Œæ¥è¿‘èˆ¹é•¿æ°´å¹³ï¼Œç»§ç»­ä¼˜åŒ–"
    elif captain_score >= 40:
        grade = "ä¸€èˆ¬"
        advice = "é£æ ¼æœ‰èˆ¹é•¿å½±å­ï¼Œä½†è¿˜éœ€åŠ å¼º"
    else:
        grade = "è¾ƒå·®"
        advice = "ä¸èˆ¹é•¿é£æ ¼å·®å¼‚è¾ƒå¤§ï¼Œå»ºè®®å­¦ä¹ èˆ¹é•¿æ¨¡æ¿"

    return {
        "status": "success",
        "article_count": len(articles),
        "captain_score": round(captain_score, 2),
        "grade": grade,
        "advice": advice,
        "dimensions": {
            "å¼€å¤´ç­–ç•¥": opening_strategy,
            "å†…å®¹ç»“æ„": content_structure,
            "æ•°æ®æ”¯æ’‘": data_support,
            "è¯­è¨€é£æ ¼": language_style,
            "ç»“å°¾è®¾è®¡": ending_design,
        },
        "suggestions": _generate_captain_suggestions({
            "å¼€å¤´ç­–ç•¥": opening_strategy,
            "å†…å®¹ç»“æ„": content_structure,
            "æ•°æ®æ”¯æ’‘": data_support,
            "è¯­è¨€é£æ ¼": language_style,
            "ç»“å°¾è®¾è®¡": ending_design,
        })
    }


def _analyze_captain_opening(articles: List[Dict[str, str]]) -> Dict[str, Any]:
    """åˆ†æèˆ¹é•¿å¼å¼€å¤´ç­–ç•¥"""
    score = 0
    features = []

    # æ£€æŸ¥çƒ­ç‚¹å¼•å…¥
    hot_topic_keywords = ["Sora2", "AIè§†é¢‘", "Nano Banana", "Midjourney", "ChatGPT"]
    has_hot_topic = 0
    for article in articles:
        content = article["content"][:200]  # åªæ£€æŸ¥å‰200å­—
        if any(keyword in content for keyword in hot_topic_keywords):
            has_hot_topic += 1

    hot_topic_ratio = has_hot_topic / len(articles)
    if hot_topic_ratio >= 0.5:
        score += 25
        features.append(f"çƒ­ç‚¹å¼•å…¥ç‡ï¼š{hot_topic_ratio:.0%}ï¼ˆä¼˜ç§€ï¼‰")
    elif hot_topic_ratio >= 0.3:
        score += 20
        features.append(f"çƒ­ç‚¹å¼•å…¥ç‡ï¼š{hot_topic_ratio:.0%}ï¼ˆè‰¯å¥½ï¼‰")
    else:
        score += 10
        features.append(f"çƒ­ç‚¹å¼•å…¥ç‡ï¼š{hot_topic_ratio:.0%}ï¼ˆéœ€åŠ å¼ºï¼‰")

    # æ£€æŸ¥ç—›ç‚¹å…±é¸£
    pain_point_keywords = ["é—®é¢˜", "ç—›ç‚¹", "å›°æ‰°", "çƒ¦æ¼", "å›°éš¾", "æŒ‘æˆ˜"]
    has_pain_point = 0
    for article in articles:
        content = article["content"][:200]
        if any(keyword in content for keyword in pain_point_keywords):
            has_pain_point += 1

    pain_point_ratio = has_pain_point / len(articles)
    if pain_point_ratio >= 0.4:
        score += 25
        features.append(f"ç—›ç‚¹å…±é¸£ç‡ï¼š{pain_point_ratio:.0%}ï¼ˆä¼˜ç§€ï¼‰")
    elif pain_point_ratio >= 0.2:
        score += 15
        features.append(f"ç—›ç‚¹å…±é¸£ç‡ï¼š{pain_point_ratio:.0%}ï¼ˆè‰¯å¥½ï¼‰")
    else:
        score += 5
        features.append(f"ç—›ç‚¹å…±é¸£ç‡ï¼š{pain_point_ratio:.0%}ï¼ˆéœ€åŠ å¼ºï¼‰")

    # æ£€æŸ¥ç¨€ç¼ºæ€§å£°æ˜
    scarcity_keywords = ["é¦–å‘", "ç‹¬å®¶", "å…è´¹", "æ— é™", "é™æ—¶", "æœ€å", "æ‰‹æ…¢æ— "]
    has_scarcity = 0
    for article in articles:
        content = article["content"][:200]
        if any(keyword in content for keyword in scarcity_keywords):
            has_scarcity += 1

    scarcity_ratio = has_scarcity / len(articles)
    if scarcity_ratio >= 0.6:
        score += 25
        features.append(f"ç¨€ç¼ºæ€§å£°æ˜ç‡ï¼š{scarcity_ratio:.0%}ï¼ˆä¼˜ç§€ï¼‰")
    elif scarcity_ratio >= 0.3:
        score += 15
        features.append(f"ç¨€ç¼ºæ€§å£°æ˜ç‡ï¼š{scarcity_ratio:.0%}ï¼ˆè‰¯å¥½ï¼‰")
    else:
        score += 5
        features.append(f"ç¨€ç¼ºæ€§å£°æ˜ç‡ï¼š{scarcity_ratio:.0%}ï¼ˆéœ€åŠ å¼ºï¼‰")

    # æ£€æŸ¥"å¼€å¹•é›·å‡»"æ•ˆæœï¼ˆå‰3è¡Œæœ‰å†²å‡»åŠ›ï¼‰
    opening_lines_avg_length = 0
    for article in articles:
        lines = article["content"].split('\n')[:3]
        opening_lines_avg_length += sum(len(line) for line in lines) / 3
    opening_lines_avg_length /= len(articles)

    if opening_lines_avg_length >= 15 and opening_lines_avg_length <= 30:
        score += 25
        features.append(f"å¼€å¤´å¹³å‡é•¿åº¦ï¼š{opening_lines_avg_length:.1f}å­—ï¼ˆé€‚ä¸­ï¼‰")
    else:
        score += 10
        features.append(f"å¼€å¤´å¹³å‡é•¿åº¦ï¼š{opening_lines_avg_length:.1f}å­—ï¼ˆéœ€ä¼˜åŒ–ï¼‰")

    return {
        "score": score,
        "features": features,
        "details": {
            "çƒ­ç‚¹å¼•å…¥ç‡": hot_topic_ratio,
            "ç—›ç‚¹å…±é¸£ç‡": pain_point_ratio,
            "ç¨€ç¼ºæ€§å£°æ˜ç‡": scarcity_ratio,
            "å¼€å¤´å¹³å‡é•¿åº¦": opening_lines_avg_length,
        }
    }


def _analyze_captain_structure(articles: List[Dict[str, str]]) -> Dict[str, Any]:
    """åˆ†æèˆ¹é•¿å¼å†…å®¹ç»“æ„ï¼ˆå››æ®µå¼ï¼šä»‹ç»â†’å¯¹æ¯”â†’ä½“éªŒâ†’å»ºè®®ï¼‰"""
    score = 0
    features = []

    # æ£€æŸ¥æ˜¯å¦æœ‰æ˜ç¡®çš„åˆ†æ®µæ ‡è®°
    has_sections = 0
    for article in articles:
        content = article["content"]
        # æ£€æŸ¥æ˜¯å¦æœ‰å°æ ‡é¢˜æˆ–åˆ†æ®µ
        if re.search(r'#{1,2}\s', content) or re.search(r'ç¬¬[ä¸€äºŒä¸‰å››]', content):
            has_sections += 1

    section_ratio = has_sections / len(articles)
    if section_ratio >= 0.8:
        score += 30
        features.append(f"åˆ†æ®µæ¸…æ™°ç‡ï¼š{section_ratio:.0%}ï¼ˆä¼˜ç§€ï¼‰")
    elif section_ratio >= 0.5:
        score += 20
        features.append(f"åˆ†æ®µæ¸…æ™°ç‡ï¼š{section_ratio:.0%}ï¼ˆè‰¯å¥½ï¼‰")
    else:
        score += 10
        features.append(f"åˆ†æ®µæ¸…æ™°ç‡ï¼š{section_ratio:.0%}ï¼ˆéœ€åŠ å¼ºï¼‰")

    # æ£€æŸ¥æ˜¯å¦æœ‰å¯¹æ¯”å†…å®¹
    comparison_keywords = ["å¯¹æ¯”", "æµ‹è¯•", "æµ‹è¯„", "æ¯”è¾ƒ", "ä¸æ¯”", "åª²ç¾"]
    has_comparison = 0
    for article in articles:
        if any(keyword in article["content"] for keyword in comparison_keywords):
            has_comparison += 1

    comparison_ratio = has_comparison / len(articles)
    if comparison_ratio >= 0.5:
        score += 30
        features.append(f"å¯¹æ¯”å†…å®¹ç‡ï¼š{comparison_ratio:.0%}ï¼ˆä¼˜ç§€ï¼‰")
    elif comparison_ratio >= 0.3:
        score += 20
        features.append(f"å¯¹æ¯”å†…å®¹ç‡ï¼š{comparison_ratio:.0%}ï¼ˆè‰¯å¥½ï¼‰")
    else:
        score += 5
        features.append(f"å¯¹æ¯”å†…å®¹ç‡ï¼š{comparison_ratio:.0%}ï¼ˆéœ€åŠ å¼ºï¼‰")

    # æ£€æŸ¥æ˜¯å¦æœ‰å®ç”¨å»ºè®®
    advice_keywords = ["å»ºè®®", "æ¨è", "æŠ€å·§", "æ–¹æ³•", "æ­¥éª¤", "æ•™ç¨‹"]
    has_advice = 0
    for article in articles:
        if any(keyword in article["content"] for keyword in advice_keywords):
            has_advice += 1

    advice_ratio = has_advice / len(articles)
    if advice_ratio >= 0.7:
        score += 40
        features.append(f"å®ç”¨å»ºè®®ç‡ï¼š{advice_ratio:.0%}ï¼ˆä¼˜ç§€ï¼‰")
    elif advice_ratio >= 0.4:
        score += 25
        features.append(f"å®ç”¨å»ºè®®ç‡ï¼š{advice_ratio:.0%}ï¼ˆè‰¯å¥½ï¼‰")
    else:
        score += 10
        features.append(f"å®ç”¨å»ºè®®ç‡ï¼š{advice_ratio:.0%}ï¼ˆéœ€åŠ å¼ºï¼‰")

    return {
        "score": score,
        "features": features,
        "details": {
            "åˆ†æ®µæ¸…æ™°ç‡": section_ratio,
            "å¯¹æ¯”å†…å®¹ç‡": comparison_ratio,
            "å®ç”¨å»ºè®®ç‡": advice_ratio,
        }
    }


def _analyze_captain_data_support(articles: List[Dict[str, str]]) -> Dict[str, Any]:
    """åˆ†ææ•°æ®æ”¯æ’‘ï¼ˆæ¡ˆä¾‹å¯†åº¦ã€æ•°æ®å¼•ç”¨ï¼‰"""
    score = 0
    features = []

    # æ£€æŸ¥æ¡ˆä¾‹å¯†åº¦
    case_keywords = ["æ¡ˆä¾‹", "ç¤ºä¾‹", "æ¯”å¦‚", "ä¾‹å¦‚", "æ¼”ç¤º", "å®æµ‹"]
    total_cases = 0
    for article in articles:
        case_count = sum(article["content"].count(keyword) for keyword in case_keywords)
        total_cases += case_count

    avg_cases_per_article = total_cases / len(articles)
    if avg_cases_per_article >= 5:
        score += 50
        features.append(f"å¹³å‡æ¡ˆä¾‹æ•°ï¼š{avg_cases_per_article:.1f}ä¸ª/ç¯‡ï¼ˆä¼˜ç§€ï¼‰")
    elif avg_cases_per_article >= 3:
        score += 35
        features.append(f"å¹³å‡æ¡ˆä¾‹æ•°ï¼š{avg_cases_per_article:.1f}ä¸ª/ç¯‡ï¼ˆè‰¯å¥½ï¼‰")
    else:
        score += 15
        features.append(f"å¹³å‡æ¡ˆä¾‹æ•°ï¼š{avg_cases_per_article:.1f}ä¸ª/ç¯‡ï¼ˆéœ€åŠ å¼ºï¼‰")

    # æ£€æŸ¥æ•°æ®å¼•ç”¨
    data_keywords = ["%", "å€", "ä¸‡", "åƒ", "å€æ•°", "å¢é•¿", "æå‡"]
    has_data = 0
    for article in articles:
        if any(keyword in article["content"] for keyword in data_keywords):
            has_data += 1

    data_ratio = has_data / len(articles)
    if data_ratio >= 0.6:
        score += 50
        features.append(f"æ•°æ®å¼•ç”¨ç‡ï¼š{data_ratio:.0%}ï¼ˆä¼˜ç§€ï¼‰")
    elif data_ratio >= 0.3:
        score += 30
        features.append(f"æ•°æ®å¼•ç”¨ç‡ï¼š{data_ratio:.0%}ï¼ˆè‰¯å¥½ï¼‰")
    else:
        score += 10
        features.append(f"æ•°æ®å¼•ç”¨ç‡ï¼š{data_ratio:.0%}ï¼ˆéœ€åŠ å¼ºï¼‰")

    return {
        "score": score,
        "features": features,
        "details": {
            "å¹³å‡æ¡ˆä¾‹æ•°": avg_cases_per_article,
            "æ•°æ®å¼•ç”¨ç‡": data_ratio,
        }
    }


def _analyze_captain_language(articles: List[Dict[str, str]]) -> Dict[str, Any]:
    """åˆ†æè¯­è¨€é£æ ¼ï¼ˆçœŸè¯š+æ¥åœ°æ°”+é€‚åº¦æƒ…ç»ªè¯ï¼‰"""
    score = 0
    features = []

    # æ£€æŸ¥çœŸè¯šåº¦ï¼ˆä¸ªäººç»å†åˆ†äº«ï¼‰
    personal_keywords = ["æˆ‘", "æˆ‘çš„", "äº²èº«", "å®æµ‹", "ç»éªŒ", "åˆ†äº«"]
    has_personal = 0
    for article in articles:
        if any(keyword in article["content"] for keyword in personal_keywords):
            has_personal += 1

    personal_ratio = has_personal / len(articles)
    if personal_ratio >= 0.7:
        score += 35
        features.append(f"ä¸ªäººç»å†åˆ†äº«ç‡ï¼š{personal_ratio:.0%}ï¼ˆä¼˜ç§€ï¼‰")
    elif personal_ratio >= 0.4:
        score += 25
        features.append(f"ä¸ªäººç»å†åˆ†äº«ç‡ï¼š{personal_ratio:.0%}ï¼ˆè‰¯å¥½ï¼‰")
    else:
        score += 10
        features.append(f"ä¸ªäººç»å†åˆ†äº«ç‡ï¼š{personal_ratio:.0%}ï¼ˆéœ€åŠ å¼ºï¼‰")

    # æ£€æŸ¥æ¥åœ°æ°”ï¼ˆå£è¯­åŒ–è¡¨è¾¾ï¼‰
    colloquial_keywords = ["å§", "å‘¢", "å“¦", "å•Š", "å˜›", "å“ˆ"]
    total_colloquial = 0
    for article in articles:
        colloquial_count = sum(article["content"].count(keyword) for keyword in colloquial_keywords)
        total_colloquial += colloquial_count

    avg_colloquial_per_article = total_colloquial / len(articles)
    if avg_colloquial_per_article >= 10 and avg_colloquial_per_article <= 30:
        score += 35
        features.append(f"å£è¯­åŒ–è¡¨è¾¾ï¼š{avg_colloquial_per_article:.1f}æ¬¡/ç¯‡ï¼ˆé€‚ä¸­ï¼‰")
    elif avg_colloquial_per_article >= 5:
        score += 20
        features.append(f"å£è¯­åŒ–è¡¨è¾¾ï¼š{avg_colloquial_per_article:.1f}æ¬¡/ç¯‡ï¼ˆè‰¯å¥½ï¼‰")
    else:
        score += 10
        features.append(f"å£è¯­åŒ–è¡¨è¾¾ï¼š{avg_colloquial_per_article:.1f}æ¬¡/ç¯‡ï¼ˆéœ€ä¼˜åŒ–ï¼‰")

    # æ£€æŸ¥é€‚åº¦æƒ…ç»ªè¯
    emotion_keywords = ["æ¿€åŠ¨", "æƒŠå–œ", "éœ‡æ’¼", "å¤ªæ£’äº†", "å¹²è´§", "ç‚¸è£‚"]
    has_emotion = 0
    for article in articles:
        if any(keyword in article["content"] for keyword in emotion_keywords):
            has_emotion += 1

    emotion_ratio = has_emotion / len(articles)
    if emotion_ratio >= 0.3 and emotion_ratio <= 0.6:
        score += 30
        features.append(f"æƒ…ç»ªè¯ä½¿ç”¨ç‡ï¼š{emotion_ratio:.0%}ï¼ˆé€‚ä¸­ï¼‰")
    elif emotion_ratio >= 0.1:
        score += 20
        features.append(f"æƒ…ç»ªè¯ä½¿ç”¨ç‡ï¼š{emotion_ratio:.0%}ï¼ˆè‰¯å¥½ï¼‰")
    else:
        score += 10
        features.append(f"æƒ…ç»ªè¯ä½¿ç”¨ç‡ï¼š{emotion_ratio:.0%}ï¼ˆéœ€åŠ å¼ºï¼‰")

    return {
        "score": score,
        "features": features,
        "details": {
            "ä¸ªäººç»å†åˆ†äº«ç‡": personal_ratio,
            "å£è¯­åŒ–è¡¨è¾¾": avg_colloquial_per_article,
            "æƒ…ç»ªè¯ä½¿ç”¨ç‡": emotion_ratio,
        }
    }


def _analyze_captain_ending(articles: List[Dict[str, str]]) -> Dict[str, Any]:
    """åˆ†æç»“å°¾è®¾è®¡ï¼ˆäº’åŠ¨å¼•å¯¼+ç§åŸŸè½¬åŒ–ï¼‰"""
    score = 0
    features = []

    # æ£€æŸ¥äº’åŠ¨å¼•å¯¼
    interaction_keywords = ["ç•™è¨€", "è¯„è®º", "ç§ä¿¡", "æ‰“èµ", "å…³æ³¨"]
    has_interaction = 0
    for article in articles:
        # æ£€æŸ¥æ–‡ç« ç»“å°¾ï¼ˆæœ€å200å­—ï¼‰
        ending = article["content"][-200:]
        if any(keyword in ending for keyword in interaction_keywords):
            has_interaction += 1

    interaction_ratio = has_interaction / len(articles)
    if interaction_ratio >= 0.8:
        score += 50
        features.append(f"äº’åŠ¨å¼•å¯¼ç‡ï¼š{interaction_ratio:.0%}ï¼ˆä¼˜ç§€ï¼‰")
    elif interaction_ratio >= 0.5:
        score += 35
        features.append(f"äº’åŠ¨å¼•å¯¼ç‡ï¼š{interaction_ratio:.0%}ï¼ˆè‰¯å¥½ï¼‰")
    else:
        score += 15
        features.append(f"äº’åŠ¨å¼•å¯¼ç‡ï¼š{interaction_ratio:.0%}ï¼ˆéœ€åŠ å¼ºï¼‰")

    # æ£€æŸ¥ç´§è¿«æ„Ÿå¼ºåŒ–
    urgency_keywords = ["èµ¶ç´§", "ç«‹å³", "é©¬ä¸Š", "æ‰‹æ…¢æ— ", "åˆ«é”™è¿‡", "è½¬å‘"]
    has_urgency = 0
    for article in articles:
        ending = article["content"][-200:]
        if any(keyword in ending for keyword in urgency_keywords):
            has_urgency += 1

    urgency_ratio = has_urgency / len(articles)
    if urgency_ratio >= 0.6:
        score += 50
        features.append(f"ç´§è¿«æ„Ÿå¼ºåŒ–ç‡ï¼š{urgency_ratio:.0%}ï¼ˆä¼˜ç§€ï¼‰")
    elif urgency_ratio >= 0.3:
        score += 30
        features.append(f"ç´§è¿«æ„Ÿå¼ºåŒ–ç‡ï¼š{urgency_ratio:.0%}ï¼ˆè‰¯å¥½ï¼‰")
    else:
        score += 10
        features.append(f"ç´§è¿«æ„Ÿå¼ºåŒ–ç‡ï¼š{urgency_ratio:.0%}ï¼ˆéœ€åŠ å¼ºï¼‰")

    return {
        "score": score,
        "features": features,
        "details": {
            "äº’åŠ¨å¼•å¯¼ç‡": interaction_ratio,
            "ç´§è¿«æ„Ÿå¼ºåŒ–ç‡": urgency_ratio,
        }
    }


def _generate_captain_suggestions(dimensions: Dict[str, Any]) -> List[str]:
    """ç”Ÿæˆèˆ¹é•¿å¼é£æ ¼ä¼˜åŒ–å»ºè®®"""
    suggestions = []

    # å¼€å¤´ç­–ç•¥å»ºè®®
    opening = dimensions["å¼€å¤´ç­–ç•¥"]
    if opening["score"] < 80:
        if opening["details"]["çƒ­ç‚¹å¼•å…¥ç‡"] < 0.5:
            suggestions.append("å¼€å¤´ç­–ç•¥ï¼šå»ºè®®å¢åŠ çƒ­ç‚¹å·¥å…·å¼•å…¥ï¼ˆSora2ã€Midjourneyç­‰ï¼‰")
        if opening["details"]["ç¨€ç¼ºæ€§å£°æ˜ç‡"] < 0.3:
            suggestions.append("å¼€å¤´ç­–ç•¥ï¼šå»ºè®®å¢åŠ ç¨€ç¼ºæ€§å£°æ˜ï¼ˆå…è´¹ã€æ— é™ã€é¦–å‘ï¼‰")

    # å†…å®¹ç»“æ„å»ºè®®
    structure = dimensions["å†…å®¹ç»“æ„"]
    if structure["score"] < 80:
        if structure["details"]["åˆ†æ®µæ¸…æ™°ç‡"] < 0.5:
            suggestions.append("å†…å®¹ç»“æ„ï¼šå»ºè®®ä½¿ç”¨å°æ ‡é¢˜åˆ†æ®µï¼Œå››æ®µå¼ç»“æ„ï¼ˆä»‹ç»â†’å¯¹æ¯”â†’ä½“éªŒâ†’å»ºè®®ï¼‰")
        if structure["details"]["å¯¹æ¯”å†…å®¹ç‡"] < 0.3:
            suggestions.append("å†…å®¹ç»“æ„ï¼šå»ºè®®å¢åŠ ç«å“å¯¹æ¯”å†…å®¹ï¼ˆä¸æ¯”XXå·®ã€åª²ç¾XXï¼‰")

    # æ•°æ®æ”¯æ’‘å»ºè®®
    data_support = dimensions["æ•°æ®æ”¯æ’‘"]
    if data_support["score"] < 80:
        if data_support["details"]["å¹³å‡æ¡ˆä¾‹æ•°"] < 3:
            suggestions.append("æ•°æ®æ”¯æ’‘ï¼šå»ºè®®å¢åŠ çœŸå®æ¡ˆä¾‹ï¼ˆå®æµ‹ã€æ¼”ç¤ºã€ç¤ºä¾‹ï¼‰")
        if data_support["details"]["æ•°æ®å¼•ç”¨ç‡"] < 0.3:
            suggestions.append("æ•°æ®æ”¯æ’‘ï¼šå»ºè®®å¢åŠ æ•°æ®å¼•ç”¨ï¼ˆç™¾åˆ†æ¯”ã€å€æ•°ã€å¢é•¿ï¼‰")

    # è¯­è¨€é£æ ¼å»ºè®®
    language = dimensions["è¯­è¨€é£æ ¼"]
    if language["score"] < 80:
        if language["details"]["ä¸ªäººç»å†åˆ†äº«ç‡"] < 0.4:
            suggestions.append("è¯­è¨€é£æ ¼ï¼šå»ºè®®å¢åŠ ä¸ªäººç»å†åˆ†äº«ï¼ˆæˆ‘å®æµ‹ã€æˆ‘ä½¿ç”¨ã€æˆ‘çš„ç»éªŒï¼‰")
        if language["details"]["å£è¯­åŒ–è¡¨è¾¾"] < 5:
            suggestions.append("è¯­è¨€é£æ ¼ï¼šå»ºè®®ä½¿ç”¨æ›´æ¥åœ°æ°”çš„å£è¯­åŒ–è¡¨è¾¾")

    # ç»“å°¾è®¾è®¡å»ºè®®
    ending = dimensions["ç»“å°¾è®¾è®¡"]
    if ending["score"] < 80:
        if ending["details"]["äº’åŠ¨å¼•å¯¼ç‡"] < 0.5:
            suggestions.append("ç»“å°¾è®¾è®¡ï¼šå»ºè®®å¢åŠ äº’åŠ¨å¼•å¯¼ï¼ˆç•™è¨€ã€ç§ä¿¡ã€æ‰“èµã€å…³æ³¨ï¼‰")
        if ending["details"]["ç´§è¿«æ„Ÿå¼ºåŒ–ç‡"] < 0.3:
            suggestions.append("ç»“å°¾è®¾è®¡ï¼šå»ºè®®å¢åŠ ç´§è¿«æ„Ÿå¼ºåŒ–ï¼ˆèµ¶ç´§ã€ç«‹å³ã€æ‰‹æ…¢æ— ã€è½¬å‘ï¼‰")

    return suggestions if suggestions else ["å·²ç»è¾¾åˆ°äº†èˆ¹é•¿çš„é£æ ¼æ ‡å‡†ï¼"]


def generate_captain_style_prompt() -> str:
    """
    ç”Ÿæˆèˆ¹é•¿å¼é£æ ¼Prompt

    åŸºäºèˆ¹é•¿AIè§†ç•Œçš„å†™ä½œé£æ ¼ï¼Œç”Ÿæˆå¯å¤ç”¨çš„é£æ ¼Prompt

    Returns:
        èˆ¹é•¿å¼é£æ ¼Prompt
    """
    prompt = """# èˆ¹é•¿å¼å†™ä½œé£æ ¼æŒ‡å—

## å¼€ç¯‡ç­–ç•¥ï¼ˆå‰200å­—ï¼‰

### 1. çƒ­ç‚¹å¼•å…¥ï¼ˆå‰3è¡Œï¼‰
- å¿…é¡»åŒ…å«çƒ­ç‚¹å·¥å…·åï¼šSora2ã€Nano Banana Proã€Midjourneyã€ChatGPTç­‰
- çªå‡ºæ—¶æ•ˆæ€§ï¼š"å…¨ç½‘é¦–å‘"ã€"åˆšåˆšå‘ç°"ã€"æœ€æ–°"

### 2. ç—›ç‚¹å…±é¸£ï¼ˆ100å­—å†…ï¼‰
- æè¿°ç”¨æˆ·é‡åˆ°çš„é—®é¢˜æˆ–å›°æƒ‘
- ä½¿ç”¨é—®å¥æˆ–æ„Ÿå¹å¥å¢å¼ºå†²å‡»åŠ›

### 3. ç¨€ç¼ºæ€§å£°æ˜ï¼ˆ200å­—å†…ï¼‰
- å¼ºè°ƒ"å…è´¹"ã€"æ— é™"ã€"é¦–å‘"
- åˆ¶é€ ç´§è¿«æ„Ÿï¼š"ä¸€ä¸¤å¤©åå¯èƒ½å°±ä¸è¡Œäº†"

## å†…å®¹ç»“æ„ï¼ˆå››æ®µå¼ï¼‰

### 1. é¡¹ç›®/å·¥å…·ä»‹ç»
- çœŸå®ç»å†èƒŒä¹¦ï¼š"æˆ‘å®æµ‹äº†XXå¤©"
- è·å¥–æˆ–æˆå°±å±•ç¤ºï¼š"å¸®åŠ©äº†XXäºº"

### 2. æŠ€æœ¯å¯¹æ¯”
- æ¨ªå‘å¯¹æ¯”ç«å“ï¼šä¸æ¯”Sora2å·®ã€åª²ç¾Midjourney
- æ•°æ®æ”¯æ’‘ï¼šæå‡XX%ã€XXå€å¢é•¿

### 3. æ·±åº¦ä½“éªŒ
- å…·ä½“ä½¿ç”¨åœºæ™¯
- æ“ä½œæ­¥éª¤æ¼”ç¤º
- çœŸå®æ¡ˆä¾‹å±•ç¤º

### 4. å®ç”¨å»ºè®®
- é€‰å‹æŒ‡å—
- ä½¿ç”¨æŠ€å·§
- æœªæ¥å±•æœ›

## è¯­è¨€é£æ ¼

### 1. çœŸè¯šåˆ†äº«
- å¤šç”¨ç¬¬ä¸€äººç§°ï¼š"æˆ‘"ã€"æˆ‘çš„"ã€"äº²èº«"
- åˆ†äº«çœŸå®ç»å†ï¼š"æˆ‘å®æµ‹"ã€"æˆ‘ä½¿ç”¨"

### 2. æ¥åœ°æ°”
- é€‚åº¦å£è¯­åŒ–ï¼šå§ã€å‘¢ã€å“¦ã€å•Šã€å˜›
- é¿å…ä¸“ä¸šæœ¯è¯­å †ç Œ

### 3. é€‚åº¦æƒ…ç»ªè¯
- "æ¿€åŠ¨"ã€"æƒŠå–œ"ã€"éœ‡æ’¼"ã€"å¤ªæ£’äº†"
- "å¹²è´§"ã€"ç‚¸è£‚"ã€"ç»äº†"

### 4. äººè®¾è¯
- ç»Ÿä¸€è‡ªç§°ï¼š"èˆ¹é•¿"ã€"èˆ¹å‘˜"
- "æ‰‹æŠŠæ‰‹æ•™ä½ "ã€"å®Œæ•´å¹²è´§"

## ç»“å°¾è®¾è®¡ï¼ˆæœ€å200å­—ï¼‰

### 1. äº’åŠ¨å¼•å¯¼
- "æœ‰ä¸æ‡‚çš„å¯ä»¥ç•™è¨€"
- "å…¶ä»–èˆ¹å‘˜ä¹Ÿä¼šå›å¤"
- "æ¬¢è¿ç§ä¿¡äº¤æµ"

### 2. ç§åŸŸè½¬åŒ–
- "è¯„è®ºåŒºæˆ‘ä¼šæ”¾åœ°å€"
- "ç§ä¿¡æˆ‘ï¼Œè·å–ç§˜ç±"
- "æ‰“èµæ”¯æŒï¼ŒæŒç»­è¾“å‡º"

### 3. ç´§è¿«æ„Ÿå¼ºåŒ–
- "èµ¶ç´§ç”¨"
- "æ‰‹æ…¢æ— "
- "è½¬å‘ç»™éœ€è¦çš„äºº"

## æ•°æ®æ”¯æ’‘è¦æ±‚

- **æ¡ˆä¾‹å¯†åº¦**ï¼šæ¯ç¯‡è‡³å°‘3-5ä¸ªçœŸå®æ¡ˆä¾‹
- **æ•°æ®å¼•ç”¨**ï¼šä½¿ç”¨ç™¾åˆ†æ¯”ã€å€æ•°ã€å¢é•¿ç­‰æ•°æ®
- **å¯¹æ¯”æµ‹è¯•**ï¼šä¸ç«å“è¿›è¡Œå¯¹æ¯”æµ‹è¯•

## æ’ç‰ˆè¦æ±‚

- ä½¿ç”¨å°æ ‡é¢˜åˆ†æ®µ
- æ¯æ®µä¸è¶…è¿‡5è¡Œ
- å…³é”®ä¿¡æ¯åŠ ç²—
- é…åˆæˆªå›¾å±•ç¤º

## æ³¨æ„äº‹é¡¹

1. **é¿å…è¿‡åº¦å¤¸å¼ **ï¼šçœŸå®æ¯”å¤¸å¼ æ›´æœ‰è¯´æœåŠ›
2. **ä¿æŒä¸€è‡´æ€§**ï¼šæ¯æ¬¡éƒ½ä½¿ç”¨ç›¸åŒçš„è‡ªç§°
3. **æŒç»­äº’åŠ¨**ï¼šå›å¤æ¯ä¸€æ¡ç•™è¨€
4. **ä»·å€¼ä¸ºç‹**ï¼šå†…å®¹å¿…é¡»çœŸæ­£æœ‰ç”¨

---

*åŸºäºèˆ¹é•¿AIè§†ç•Œ100ç¯‡æ–‡ç« åˆ†æç”Ÿæˆ*
*é€‚ç”¨äºAIå·¥å…·æµ‹è¯„ç±»å†…å®¹åˆ›ä½œ*
"""

    return prompt


def evaluate_captain_similarity(articles_text: str, captain_articles_text: str = None) -> Dict[str, Any]:
    """
    è¯„ä¼°æ–‡ç« ä¸èˆ¹é•¿é£æ ¼çš„ç›¸ä¼¼åº¦

    Args:
        articles_text: å¾…è¯„ä¼°çš„æ–‡ç« æ–‡æœ¬
        captain_articles_text: èˆ¹é•¿çš„æ–‡ç« æ–‡æœ¬ï¼ˆå¯é€‰ï¼Œå¦‚ä¸æä¾›åˆ™ä½¿ç”¨æ ‡å‡†æ¨¡æ¿ï¼‰

    Returns:
        ç›¸ä¼¼åº¦è¯„ä¼°ç»“æœ
    """
    # åˆ†æå¾…è¯„ä¼°æ–‡ç« 
    user_analysis = analyze_captain_style(articles_text)

    # æ ‡å‡†èˆ¹é•¿é£æ ¼åŸºå‡†åˆ†
    standard_scores = {
        "å¼€å¤´ç­–ç•¥": 80,
        "å†…å®¹ç»“æ„": 80,
        "æ•°æ®æ”¯æ’‘": 80,
        "è¯­è¨€é£æ ¼": 80,
        "ç»“å°¾è®¾è®¡": 80,
    }

    # è®¡ç®—ç›¸ä¼¼åº¦
    similarities = {}
    for dimension in standard_scores.keys():
        user_score = user_analysis["dimensions"][dimension]["score"]
        standard_score = standard_scores[dimension]
        similarity = min(user_score / standard_score, 1.0)
        similarities[dimension] = round(similarity * 100, 2)

    # è®¡ç®—æ€»ä½“ç›¸ä¼¼åº¦
    overall_similarity = sum(similarities.values()) / len(similarities)

    # åˆ¤æ–­ç­‰çº§
    if overall_similarity >= 80:
        grade = "é«˜åº¦ç›¸ä¼¼ï¼ˆèˆ¹é•¿çº§åˆ«ï¼‰"
        advice = "æ‚¨çš„å†™ä½œé£æ ¼å·²ç»éå¸¸æ¥è¿‘èˆ¹é•¿ï¼"
    elif overall_similarity >= 60:
        grade = "ä¸­åº¦ç›¸ä¼¼"
        advice = "æ‚¨çš„å†™ä½œé£æ ¼æœ‰èˆ¹é•¿å½±å­ï¼Œç»§ç»­ä¼˜åŒ–ï¼"
    elif overall_similarity >= 40:
        grade = "è½»åº¦ç›¸ä¼¼"
        advice = "æ‚¨çš„å†™ä½œé£æ ¼ä¸èˆ¹é•¿æœ‰ä¸€å®šå·®å¼‚ï¼Œå»ºè®®å‚è€ƒèˆ¹é•¿æ¨¡æ¿"
    else:
        grade = "ä¸ç›¸ä¼¼"
        advice = "æ‚¨çš„å†™ä½œé£æ ¼ä¸èˆ¹é•¿å·®å¼‚è¾ƒå¤§ï¼Œå»ºè®®å­¦ä¹ èˆ¹é•¿æ¨¡æ¿"

    return {
        "status": "success",
        "overall_similarity": round(overall_similarity, 2),
        "grade": grade,
        "advice": advice,
        "dimension_similarities": similarities,
        "user_score": user_analysis["captain_score"],
        "suggestions": user_analysis["suggestions"]
    }


def handler(args: Dict[str, Any]) -> Dict[str, Any]:
    """
    ä¸»å¤„ç†å‡½æ•°

    Args:
        args: åŒ…å«ä»¥ä¸‹å­—æ®µçš„å­—å…¸
            - action: æ“ä½œç±»å‹ï¼ˆanalyze_style/generate_style_prompt/generate_title/polish_content/create_articleï¼‰
            - articles: æ–‡ç« æ–‡æœ¬ï¼ˆanalyze_styleæ—¶å¿…éœ€ï¼‰
            - style_file: é£æ ¼åˆ†ææ–‡ä»¶è·¯å¾„ï¼ˆå…¶ä»–æ“ä½œæ—¶å¿…éœ€ï¼‰
            - topic: ä¸»é¢˜ï¼ˆgenerate_titleæ—¶å¿…éœ€ï¼‰
            - limit: ç”Ÿæˆæ•°é‡ï¼ˆå¯é€‰ï¼Œé»˜è®¤10ï¼‰
            - output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            - content: è¦æ¶¦è‰²çš„å†…å®¹ï¼ˆpolish_contentæ—¶å¿…éœ€ï¼‰
            - outline: æ–‡ç« å¤§çº²ï¼ˆcreate_articleæ—¶å¿…éœ€ï¼‰

    Returns:
        å¤„ç†ç»“æœ
    """
    action = args.get("action", "")

    if action == "analyze_style":
        articles = args.get("articles", "")
        output_file = args.get("output_file", "style_analysis.json")

        if not articles:
            return {
                "status": "error",
                "message": "æ–‡ç« å†…å®¹ä¸èƒ½ä¸ºç©º"
            }

        # åˆ†æé£æ ¼
        result = analyze_style(articles)

        # ä¿å­˜ç»“æœ
        if output_file:
            save_to_file(result, output_file)

        return result

    elif action == "generate_style_prompt":
        style_file = args.get("style_file", "")
        output_file = args.get("output_file", "style_prompt.md")

        if not style_file:
            return {
                "status": "error",
                "message": "é£æ ¼åˆ†ææ–‡ä»¶è·¯å¾„ä¸èƒ½ä¸ºç©º"
            }

        # è¯»å–é£æ ¼åˆ†æ
        try:
            with open(style_file, 'r', encoding='utf-8') as f:
                style_analysis = json.load(f)
        except Exception as e:
            return {
                "status": "error",
                "message": f"è¯»å–é£æ ¼åˆ†ææ–‡ä»¶å¤±è´¥: {e}"
            }

        # ç”ŸæˆPrompt
        prompt = generate_style_prompt(style_analysis)

        # ä¿å­˜ç»“æœ
        if output_file:
            save_to_file(prompt, output_file)

        return {
            "status": "success",
            "prompt": prompt,
            "output_file": output_file
        }

    elif action == "generate_title":
        style_file = args.get("style_file", "")
        topic = args.get("topic", "")
        limit = args.get("limit", 10)

        if not style_file:
            return {
                "status": "error",
                "message": "é£æ ¼åˆ†ææ–‡ä»¶è·¯å¾„ä¸èƒ½ä¸ºç©º"
            }

        if not topic:
            return {
                "status": "error",
                "message": "ä¸»é¢˜ä¸èƒ½ä¸ºç©º"
            }

        # è¯»å–é£æ ¼åˆ†æ
        try:
            with open(style_file, 'r', encoding='utf-8') as f:
                style_analysis = json.load(f)
        except Exception as e:
            return {
                "status": "error",
                "message": f"è¯»å–é£æ ¼åˆ†ææ–‡ä»¶å¤±è´¥: {e}"
            }

        # ç”Ÿæˆæ ‡é¢˜
        titles = generate_titles(style_analysis, topic, limit)

        return {
            "status": "success",
            "topic": topic,
            "titles": titles
        }

    elif action == "polish_content":
        # TODO: å®ç°å†…å®¹æ¶¦è‰²åŠŸèƒ½
        return {
            "status": "error",
            "message": "å†…å®¹æ¶¦è‰²åŠŸèƒ½å¾…å®ç°"
        }

    elif action == "create_article":
        # TODO: å®ç°æ–‡ç« åˆ›ä½œåŠŸèƒ½
        return {
            "status": "error",
            "message": "æ–‡ç« åˆ›ä½œåŠŸèƒ½å¾…å®ç°"
        }

    # ===== èˆ¹é•¿å¼é£æ ¼åˆ†æï¼ˆ2026-01-15æ–°å¢ï¼‰ =====
    elif action == "analyze_captain_style":
        # åˆ†æèˆ¹é•¿å¼å†™ä½œé£æ ¼
        articles = args.get("articles", "")
        output_file = args.get("output_file", "captain_style_analysis.json")

        if not articles:
            return {
                "status": "error",
                "message": "æ–‡ç« å†…å®¹ä¸èƒ½ä¸ºç©º"
            }

        # åˆ†æèˆ¹é•¿é£æ ¼
        result = analyze_captain_style(articles)

        # ä¿å­˜ç»“æœ
        if output_file:
            save_to_file(result, output_file)

        return result

    elif action == "generate_captain_style_prompt":
        # ç”Ÿæˆèˆ¹é•¿å¼é£æ ¼Prompt
        output_file = args.get("output_file", "captain_style_prompt.md")

        # ç”ŸæˆPrompt
        prompt = generate_captain_style_prompt()

        # ä¿å­˜ç»“æœ
        if output_file:
            save_to_file(prompt, output_file)

        return {
            "status": "success",
            "prompt": prompt,
            "output_file": output_file,
            "message": "èˆ¹é•¿å¼é£æ ¼Promptå·²ç”Ÿæˆ"
        }

    elif action == "evaluate_captain_similarity":
        # è¯„ä¼°ä¸èˆ¹é•¿é£æ ¼çš„ç›¸ä¼¼åº¦
        articles = args.get("articles", "")

        if not articles:
            return {
                "status": "error",
                "message": "æ–‡ç« å†…å®¹ä¸èƒ½ä¸ºç©º"
            }

        # è¯„ä¼°ç›¸ä¼¼åº¦
        result = evaluate_captain_similarity(articles)

        return result

    else:
        return {
            "status": "error",
            "message": f"ä¸æ”¯æŒçš„æ“ä½œç±»å‹: {action}"
        }



